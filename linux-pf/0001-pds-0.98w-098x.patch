diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index 85f9b6382c7c..71e0339f543d 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -90,7 +90,7 @@ enum {
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO "pds: PDS-mq CPU Scheduler 0.98w by Alfred Chen.\n");
+	printk(KERN_INFO "pds: PDS-mq CPU Scheduler 0.98x by Alfred Chen.\n");
 }
 
 /* task_struct::on_rq states: */
@@ -405,19 +405,27 @@ static inline u64 static_deadline_diff(int static_prio)
 
 static inline struct task_struct *rq_first_queued_task(struct rq *rq)
 {
-	return skiplist_entry(rq->sl_header->next[0],
-			      struct task_struct, sl_node);
+	struct skiplist_node *node = rq->sl_header.next[0];
+
+	if (node == &rq->sl_header)
+		return rq->idle;
+
+	return skiplist_entry(node, struct task_struct, sl_node);
 }
 
 static inline struct task_struct *rq_second_queued_task(struct rq *rq)
 {
-	return skiplist_entry(rq->sl_header->next[0]->next[0],
-			      struct task_struct, sl_node);
+	struct skiplist_node *node = rq->sl_header.next[0]->next[0];
+
+	if (node == &rq->sl_header)
+		return rq->idle;
+
+	return skiplist_entry(node, struct task_struct, sl_node);
 }
 
 static inline int is_second_in_rq(struct task_struct *p, struct rq *rq)
 {
-	return (p->sl_node.prev[0]->prev[0] == rq->sl_header);
+	return (p->sl_node.prev[0]->prev[0] == &rq->sl_header);
 }
 
 static const int task_dl_hash_tbl[] = {
@@ -484,11 +492,11 @@ static inline void update_sched_rq_queued_masks_normal(struct rq *rq)
 static inline void update_sched_rq_queued_masks(struct rq *rq)
 {
 	int cpu = cpu_of(rq);
+	struct task_struct *p = rq_first_queued_task(rq);
 	unsigned long level;
 #ifdef CONFIG_SCHED_SMT
 	unsigned long last_level = rq->queued_level;
 #endif
-	struct task_struct *p = rq_first_queued_task(rq);
 
 	level = task_running_policy_level(p, rq);
 	sched_rq_prio[cpu] = p->prio;
@@ -577,7 +585,7 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq)
 
 	WARN_ONCE(task_rq(p) != rq, "pds: dequeue task reside on cpu%d from cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
-	if (skiplist_del_init(rq->sl_header, &p->sl_node)) {
+	if (skiplist_del_init(&rq->sl_header, &p->sl_node)) {
 		update_sched_rq_queued_masks(rq);
 		update_sched_rq_pending_masks(rq);
 	} else if (is_second_in_rq(p, rq))
@@ -679,7 +687,7 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq)
 		  task_cpu(p), cpu_of(rq));
 
 	p->sl_node.level = p->sl_level;
-	if (pds_skiplist_insert(rq->sl_header, &p->sl_node)) {
+	if (pds_skiplist_insert(&rq->sl_header, &p->sl_node)) {
 		update_sched_rq_queued_masks(rq);
 		update_sched_rq_pending_masks(rq);
 	} else if (is_second_in_rq(p, rq))
@@ -708,11 +716,11 @@ static inline void requeue_task(struct task_struct *p, struct rq *rq)
 	WARN_ONCE(task_rq(p) != rq, "pds: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	b_first = skiplist_del_init(rq->sl_header, &p->sl_node);
+	b_first = skiplist_del_init(&rq->sl_header, &p->sl_node);
 	b_second = is_second_in_rq(p, rq);
 
 	p->sl_node.level = p->sl_level;
-	if (pds_skiplist_insert(rq->sl_header, &p->sl_node) || b_first) {
+	if (pds_skiplist_insert(&rq->sl_header, &p->sl_node) || b_first) {
 		update_sched_rq_queued_masks(rq);
 		update_sched_rq_pending_masks(rq);
 	} else if (is_second_in_rq(p, rq) || b_second)
@@ -2949,7 +2957,7 @@ static inline bool pds_load_balance(struct rq *rq)
 	/*
 	 * this function is called when rq is locked and nr_running >= 2
 	 */
-	node = rq->sl_header->next[0]->next[0];
+	node = rq->sl_header.next[0]->next[0];
 	p = skiplist_entry(node, struct task_struct, sl_node);
 
 	/*
@@ -3275,9 +3283,9 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, int filter_prio)
 	int dest_cpu = cpu_of(dest_rq);
 	int nr_migrated = 0;
 	int nr_tries = min((rq->nr_running + 1) / 2, SCHED_RQ_NR_MIGRATION);
-	struct skiplist_node *node = rq->sl_header->next[0];
+	struct skiplist_node *node = rq->sl_header.next[0];
 
-	while (nr_tries && node != rq->sl_header) {
+	while (nr_tries && node != &rq->sl_header) {
 		p = skiplist_entry(node, struct task_struct, sl_node);
 		node = node->next[0];
 
@@ -3292,7 +3300,7 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, int filter_prio)
 		}
 		nr_tries--;
 		/* make a jump */
-		if (node == rq->sl_header)
+		if (node == &rq->sl_header)
 			break;
 		node = node->next[0];
 	}
@@ -5468,8 +5476,6 @@ void init_idle(struct task_struct *idle, int cpu)
 	raw_spin_lock(&rq->lock);
 	update_rq_clock(rq);
 
-	FULL_INIT_SKIPLIST_NODE(&idle->sl_node);
-
 	idle->last_ran = rq->clock_task;
 	idle->state = TASK_RUNNING;
 	idle->flags |= PF_IDLE;
@@ -5497,7 +5503,6 @@ void init_idle(struct task_struct *idle, int cpu)
 
 	rq->curr = rq->idle = idle;
 	idle->on_cpu = 1;
-	rq->sl_header = &idle->sl_node;
 
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock_irqrestore(&idle->pi_lock, flags);
@@ -5708,8 +5713,8 @@ static void migrate_tasks(struct rq *dead_rq)
 	 */
 	rq->stop = NULL;
 
-	node = rq->sl_header;
-	while ((node = node->next[0]) != rq->sl_header) {
+	node = &rq->sl_header;
+	while ((node = node->next[0]) != &rq->sl_header) {
 		int dest_cpu;
 
 		p = skiplist_entry(node, struct task_struct, sl_node);
@@ -5752,7 +5757,7 @@ static void migrate_tasks(struct rq *dead_rq)
 		rq = dead_rq;
 		raw_spin_lock(&rq->lock);
 		/* Check queued task all over from the header again */
-		node = rq->sl_header;
+		node = &rq->sl_header;
 	}
 
 	rq->stop = stop;
@@ -6205,9 +6210,8 @@ void __init sched_init(void)
 #endif /* CONFIG_CGROUP_SCHED */
 	for_each_possible_cpu(i) {
 		rq = cpu_rq(i);
-
+		FULL_INIT_SKIPLIST_NODE(&rq->sl_header);
 		raw_spin_lock_init(&rq->lock);
-		rq->sl_header = NULL;
 		rq->dither = 0;
 		rq->nr_running = rq->nr_uninterruptible = 0;
 		rq->calc_load_active = 0;
diff --git a/kernel/sched/pds_sched.h b/kernel/sched/pds_sched.h
index 1bc5be10919c..6d20158b123a 100644
--- a/kernel/sched/pds_sched.h
+++ b/kernel/sched/pds_sched.h
@@ -55,7 +55,7 @@ struct rq {
 	struct task_struct *curr, *idle, *stop;
 	struct mm_struct *prev_mm;
 
-	struct skiplist_node *sl_header;
+	struct skiplist_node sl_header;
 
 	/* switch count */
 	u64 nr_switches;
